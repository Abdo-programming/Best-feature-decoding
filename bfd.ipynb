{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1241d7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "import glob\n",
    "import os\n",
    "\n",
    "#models\n",
    "from smlr import SMLR as smlr\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import top_k_accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcba2b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predefine the used layers, imagenet ids, and subjects\n",
    "layers = ['conv1', 'conv2', 'conv3','conv4', 'conv5', 'fc6', 'fc7', 'fc8']\n",
    "subjs = ['Subj1', 'Subj2', 'Subj3', 'Subj4', 'Subj5']\n",
    "rois = ['VC', 'V1', 'V2', 'V3', 'V4', 'PPA', 'LOC', 'FFA']\n",
    "\n",
    "#Key dataframe converting stimulus id to imagenet id\n",
    "key = pd.read_csv('Intersecting image.csv')\n",
    "Imagenet_id = key['Imagenet_id']\n",
    "\n",
    "#Misc settings\n",
    "random_seed = 2861\n",
    "n_iter = 200\n",
    "sli = pd.IndexSlice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b460cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_extractor(roi, p, key=key, layers=layers, subjs=subjs):\n",
    "    '''\n",
    "    Function to extract data from .mat files of a single roi in all subjects\n",
    "    and combine them into a single dataframe\n",
    "    ---\n",
    "    roi: the region of interest. case sensitive to the filename in your work directory\n",
    "    p: path to the directory of subject folders\n",
    "    key: dataframe containing the key mapping of included alexnet_id to imagenet_id\n",
    "    ---\n",
    "    returns combined dataframe\n",
    "    '''\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    for mod in layers:\n",
    "        for subj in subjs:\n",
    "            #Path construction\n",
    "            path = p + mod + \"/\" + subj + \"/\" + roi + \"/\"\n",
    "            #bfd\\best_features\\1000\\conv1\\Subj1\\VC\n",
    "            files = glob.glob(os.path.join(path, \"*.mat\"))\n",
    "        \n",
    "            #Create a tmp df containing all mat data in that folder\n",
    "            tmp = pd.concat([pd.DataFrame(scipy.io.loadmat(f)['best_feature_data']) for f  in files]\n",
    "                         , ignore_index = True)\n",
    "    \n",
    "            #Add a new column to label the entries with the model name, subject number and Alexnet id\n",
    "            tmp.insert(0, 'layer', mod)\n",
    "            tmp.insert(0, 'Subject', subj)\n",
    "            \n",
    "            #Create a pd df for the index values, transforming Alexnet_id in the process\n",
    "            #e.g. from n01443537_22563 to 1443537\n",
    "            l = pd.Series([0]*len(files), dtype='int')\n",
    "            for i in range(len(files)):\n",
    "                l.iloc[i] = os.path.basename(files[i]).split('_')[0][2:]\n",
    "            l = np.repeat(l, 35)\n",
    "            \n",
    "            #Repeat the values 35 times, corresponding to 35 samples per image\n",
    "            l= pd.DataFrame(l, columns = ['Alexnet_id'])\n",
    "            tmp = pd.concat([l.reset_index(drop=True), tmp], axis = 1, ignore_index = True)\n",
    "\n",
    "            #Concatenate the resultant dfs together\n",
    "            df = pd.concat([df, tmp], axis = 0, ignore_index = True)\n",
    "    #Transform the key into a dictionary to use it in the next step\n",
    "    key.Alexnet_id = key.Alexnet_id.astype('str')\n",
    "    key = key.set_index('Alexnet_id')['Imagenet_id'].to_dict()\n",
    "    \n",
    "    #Name the metadata columns as appropriate\n",
    "    df = df.rename(columns = {0 : 'Alexnet_id', 1: 'Subject', 2: 'Layer'})\n",
    "    \n",
    "    #Create a new column, Imagenet_id in the master df, by mapping the dic with the Alexnet id values\n",
    "    df.insert(0, 'Imagenet_id', df.Alexnet_id.map(key))\n",
    "    \n",
    "    #Drop rows with nan values (these will have no associated Imagenet_id in the key. Therefore, they are not included)\n",
    "    df.dropna(inplace = True)\n",
    "    \n",
    "    #Drop the Alexnet_id column, and set the Imagenet_id column to int\n",
    "    df.drop(['Alexnet_id'], axis = 1, inplace = True)\n",
    "    df['Imagenet_id'] = df['Imagenet_id'].astype('int')\n",
    "    \n",
    "    return df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78b1f503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multi_index(x, indices):\n",
    "    '''\n",
    "    Duplicate multiple lists sequentially to be used in a multi-index dataframe\n",
    "    e.g.: \n",
    "    list = [1, 2, 3]\n",
    "    after broadcasting onto a 9-row dataframe,\n",
    "    list = [1, 2, 3, 1, 2, 3, 1, 2, 3]\n",
    "    ----\n",
    "    x: number of rows in a dataframe. Use x = 1 to calculate it by multiplying the len of lists together\n",
    "    indices: list (of lists) to be duplicated to match length x.\n",
    "    '''\n",
    "    #x is the number of rows in the dataframe; calculated by multiplying the len of lists together, or provided beforehand\n",
    "    if x == 1:\n",
    "        for i in range(len(indices)):\n",
    "            x *= len(indices[i])\n",
    "    mul_index = []\n",
    "    \n",
    "    for index in indices:\n",
    "        y = x / len(index)\n",
    "        index = list(index) * int(y)\n",
    "        mul_index += [index]\n",
    "    return mul_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5bc352",
   "metadata": {},
   "source": [
    "We have 5 subjects, with 10 ROIs per subject. Each ROI has voxel data that will be used in our experiment.\n",
    "The subjects underwent image training, followed by a perception test and an imagery test. Only the perception test data will be used in this experiment.\n",
    "\n",
    "The participants were shown 35 images for the test. Only 9 images will be selected out of those, and used for the experiment. Those were selected because their categories intersect with the ImageNet library.\n",
    "\n",
    "Pseudocode:\n",
    "1. Extract voxel, stimulus id, and data type data from each subject.\n",
    "2. Remove the training and imagery test data.\n",
    "3. Set the index to be the stimulus id, and join the dataframe with the stimulus id - imagenet id pairings.\n",
    "4. Drop rows with no image id, and drop the stimulus id, and data type columns.\n",
    "5. Split into training/test data, run Smlr and SVM, and calculate the prediction percentage.\n",
    "6. Repeat the experiment on all ROIs, and display the results in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41fa774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the pref images vectors into a single dataframe\n",
    "pref_img = pd.DataFrame()\n",
    "for mod in layers:\n",
    "    #Path in my work directory, replace with yours as necessary\n",
    "    path = 'bfd/best_features_pref_images/best_features/1000/' + mod + \"/\"\n",
    "    files = glob.glob(os.path.join(path, \"*.mat\"))\n",
    "        \n",
    "    #Create a tmp df containing all mat data in that folder, then use the index to create the imagenet_id column; same values\n",
    "    tmp = pd.concat([pd.DataFrame(scipy.io.loadmat(f)['best_feature_data']).T for f in files]\n",
    "                         , ignore_index = True).reset_index().rename(columns = {\"index\":'Imagenet_id'})\n",
    "    \n",
    "    #Add a new column to label the entries with the model name\n",
    "    tmp.insert(0, 'layer', mod)\n",
    "    \n",
    "    #Concatenate the resultant dfs together\n",
    "    pref_img = pd.concat([pref_img, tmp], axis = 0, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78197a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Imagenet_id</th>\n",
       "      <th>layer</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>conv1</td>\n",
       "      <td>11.476718</td>\n",
       "      <td>-25.895676</td>\n",
       "      <td>-40.893665</td>\n",
       "      <td>10.021077</td>\n",
       "      <td>10.557981</td>\n",
       "      <td>-36.217346</td>\n",
       "      <td>-41.286446</td>\n",
       "      <td>-75.275200</td>\n",
       "      <td>...</td>\n",
       "      <td>33.396690</td>\n",
       "      <td>82.087357</td>\n",
       "      <td>-12.189371</td>\n",
       "      <td>-119.277802</td>\n",
       "      <td>20.036339</td>\n",
       "      <td>30.214413</td>\n",
       "      <td>-13.125332</td>\n",
       "      <td>-5.200476</td>\n",
       "      <td>-6.953192</td>\n",
       "      <td>37.496685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>conv2</td>\n",
       "      <td>-262.386475</td>\n",
       "      <td>-91.425667</td>\n",
       "      <td>-236.797638</td>\n",
       "      <td>-109.876915</td>\n",
       "      <td>-132.229218</td>\n",
       "      <td>-141.268829</td>\n",
       "      <td>-87.422348</td>\n",
       "      <td>-131.869278</td>\n",
       "      <td>...</td>\n",
       "      <td>-184.656128</td>\n",
       "      <td>-181.334778</td>\n",
       "      <td>-62.832211</td>\n",
       "      <td>-155.209991</td>\n",
       "      <td>12.391937</td>\n",
       "      <td>-168.993835</td>\n",
       "      <td>-151.324615</td>\n",
       "      <td>-73.859734</td>\n",
       "      <td>-218.618866</td>\n",
       "      <td>-86.510498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>conv3</td>\n",
       "      <td>113.617676</td>\n",
       "      <td>95.916077</td>\n",
       "      <td>244.712173</td>\n",
       "      <td>1.555827</td>\n",
       "      <td>-17.988266</td>\n",
       "      <td>17.713959</td>\n",
       "      <td>154.402344</td>\n",
       "      <td>-196.257004</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.722656</td>\n",
       "      <td>95.950569</td>\n",
       "      <td>-38.018047</td>\n",
       "      <td>-34.416386</td>\n",
       "      <td>-34.806896</td>\n",
       "      <td>1.846160</td>\n",
       "      <td>-36.299171</td>\n",
       "      <td>10.153909</td>\n",
       "      <td>-7.466252</td>\n",
       "      <td>-87.169029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>conv4</td>\n",
       "      <td>98.402870</td>\n",
       "      <td>2.452147</td>\n",
       "      <td>61.812294</td>\n",
       "      <td>-23.877178</td>\n",
       "      <td>-32.050632</td>\n",
       "      <td>22.487713</td>\n",
       "      <td>-20.046038</td>\n",
       "      <td>28.367855</td>\n",
       "      <td>...</td>\n",
       "      <td>26.840666</td>\n",
       "      <td>-15.808105</td>\n",
       "      <td>-35.321037</td>\n",
       "      <td>22.377811</td>\n",
       "      <td>-1.256709</td>\n",
       "      <td>33.592144</td>\n",
       "      <td>18.019812</td>\n",
       "      <td>-34.405991</td>\n",
       "      <td>-0.702034</td>\n",
       "      <td>-62.810215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>conv5</td>\n",
       "      <td>-64.292816</td>\n",
       "      <td>-50.911831</td>\n",
       "      <td>-24.796650</td>\n",
       "      <td>-57.435677</td>\n",
       "      <td>-65.899101</td>\n",
       "      <td>-42.336655</td>\n",
       "      <td>-21.849260</td>\n",
       "      <td>-72.391479</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.734768</td>\n",
       "      <td>-26.043579</td>\n",
       "      <td>-61.206161</td>\n",
       "      <td>-32.466904</td>\n",
       "      <td>-58.548798</td>\n",
       "      <td>-80.592941</td>\n",
       "      <td>0.572763</td>\n",
       "      <td>-53.210026</td>\n",
       "      <td>11.932649</td>\n",
       "      <td>-37.241249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>897</td>\n",
       "      <td>conv4</td>\n",
       "      <td>78.928543</td>\n",
       "      <td>-66.840408</td>\n",
       "      <td>35.047390</td>\n",
       "      <td>-68.855141</td>\n",
       "      <td>-13.382402</td>\n",
       "      <td>-11.286433</td>\n",
       "      <td>-18.747446</td>\n",
       "      <td>-28.757263</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.204189</td>\n",
       "      <td>9.942905</td>\n",
       "      <td>-13.083759</td>\n",
       "      <td>-17.605408</td>\n",
       "      <td>-13.542098</td>\n",
       "      <td>-48.618340</td>\n",
       "      <td>-8.268844</td>\n",
       "      <td>-69.221214</td>\n",
       "      <td>-25.744562</td>\n",
       "      <td>16.998856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>897</td>\n",
       "      <td>conv5</td>\n",
       "      <td>-23.527922</td>\n",
       "      <td>-31.505159</td>\n",
       "      <td>-40.243137</td>\n",
       "      <td>-35.339340</td>\n",
       "      <td>-64.311089</td>\n",
       "      <td>-18.807806</td>\n",
       "      <td>5.673960</td>\n",
       "      <td>-23.250013</td>\n",
       "      <td>...</td>\n",
       "      <td>-63.854099</td>\n",
       "      <td>-12.565029</td>\n",
       "      <td>-36.956093</td>\n",
       "      <td>-51.224533</td>\n",
       "      <td>-42.695961</td>\n",
       "      <td>-13.396852</td>\n",
       "      <td>-62.568878</td>\n",
       "      <td>-81.089882</td>\n",
       "      <td>-69.127541</td>\n",
       "      <td>-26.845081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>897</td>\n",
       "      <td>fc6</td>\n",
       "      <td>-52.567741</td>\n",
       "      <td>-6.406816</td>\n",
       "      <td>-10.177212</td>\n",
       "      <td>-35.640720</td>\n",
       "      <td>-62.432510</td>\n",
       "      <td>-3.436984</td>\n",
       "      <td>-18.650530</td>\n",
       "      <td>-30.454874</td>\n",
       "      <td>...</td>\n",
       "      <td>-33.538307</td>\n",
       "      <td>-6.807866</td>\n",
       "      <td>-39.883553</td>\n",
       "      <td>-82.117966</td>\n",
       "      <td>-7.156143</td>\n",
       "      <td>-45.256710</td>\n",
       "      <td>-34.827076</td>\n",
       "      <td>1.890019</td>\n",
       "      <td>-35.873177</td>\n",
       "      <td>-10.288158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>897</td>\n",
       "      <td>fc7</td>\n",
       "      <td>0.133029</td>\n",
       "      <td>-7.150592</td>\n",
       "      <td>-3.432850</td>\n",
       "      <td>-11.783286</td>\n",
       "      <td>-8.743981</td>\n",
       "      <td>-23.279251</td>\n",
       "      <td>-8.044333</td>\n",
       "      <td>-11.834446</td>\n",
       "      <td>...</td>\n",
       "      <td>-21.521461</td>\n",
       "      <td>5.955750</td>\n",
       "      <td>-2.785596</td>\n",
       "      <td>10.657142</td>\n",
       "      <td>-7.704358</td>\n",
       "      <td>-7.466293</td>\n",
       "      <td>-4.669672</td>\n",
       "      <td>-1.998870</td>\n",
       "      <td>7.070156</td>\n",
       "      <td>31.475857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>897</td>\n",
       "      <td>fc8</td>\n",
       "      <td>1.764579</td>\n",
       "      <td>-7.636001</td>\n",
       "      <td>-13.013685</td>\n",
       "      <td>-11.339402</td>\n",
       "      <td>-4.004355</td>\n",
       "      <td>-1.804308</td>\n",
       "      <td>-9.224453</td>\n",
       "      <td>0.244433</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.243472</td>\n",
       "      <td>17.587795</td>\n",
       "      <td>20.504135</td>\n",
       "      <td>-12.424165</td>\n",
       "      <td>-5.677304</td>\n",
       "      <td>13.834739</td>\n",
       "      <td>-8.012489</td>\n",
       "      <td>-1.515846</td>\n",
       "      <td>21.004345</td>\n",
       "      <td>-11.699253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>168 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Imagenet_id  layer           0          1           2           3  \\\n",
       "0              1  conv1   11.476718 -25.895676  -40.893665   10.021077   \n",
       "1              1  conv2 -262.386475 -91.425667 -236.797638 -109.876915   \n",
       "2              1  conv3  113.617676  95.916077  244.712173    1.555827   \n",
       "3              1  conv4   98.402870   2.452147   61.812294  -23.877178   \n",
       "4              1  conv5  -64.292816 -50.911831  -24.796650  -57.435677   \n",
       "..           ...    ...         ...        ...         ...         ...   \n",
       "163          897  conv4   78.928543 -66.840408   35.047390  -68.855141   \n",
       "164          897  conv5  -23.527922 -31.505159  -40.243137  -35.339340   \n",
       "165          897    fc6  -52.567741  -6.406816  -10.177212  -35.640720   \n",
       "166          897    fc7    0.133029  -7.150592   -3.432850  -11.783286   \n",
       "167          897    fc8    1.764579  -7.636001  -13.013685  -11.339402   \n",
       "\n",
       "              4           5           6           7  ...         990  \\\n",
       "0     10.557981  -36.217346  -41.286446  -75.275200  ...   33.396690   \n",
       "1   -132.229218 -141.268829  -87.422348 -131.869278  ... -184.656128   \n",
       "2    -17.988266   17.713959  154.402344 -196.257004  ...  -19.722656   \n",
       "3    -32.050632   22.487713  -20.046038   28.367855  ...   26.840666   \n",
       "4    -65.899101  -42.336655  -21.849260  -72.391479  ...  -39.734768   \n",
       "..          ...         ...         ...         ...  ...         ...   \n",
       "163  -13.382402  -11.286433  -18.747446  -28.757263  ...  -50.204189   \n",
       "164  -64.311089  -18.807806    5.673960  -23.250013  ...  -63.854099   \n",
       "165  -62.432510   -3.436984  -18.650530  -30.454874  ...  -33.538307   \n",
       "166   -8.743981  -23.279251   -8.044333  -11.834446  ...  -21.521461   \n",
       "167   -4.004355   -1.804308   -9.224453    0.244433  ...   -4.243472   \n",
       "\n",
       "            991        992         993        994         995         996  \\\n",
       "0     82.087357 -12.189371 -119.277802  20.036339   30.214413  -13.125332   \n",
       "1   -181.334778 -62.832211 -155.209991  12.391937 -168.993835 -151.324615   \n",
       "2     95.950569 -38.018047  -34.416386 -34.806896    1.846160  -36.299171   \n",
       "3    -15.808105 -35.321037   22.377811  -1.256709   33.592144   18.019812   \n",
       "4    -26.043579 -61.206161  -32.466904 -58.548798  -80.592941    0.572763   \n",
       "..          ...        ...         ...        ...         ...         ...   \n",
       "163    9.942905 -13.083759  -17.605408 -13.542098  -48.618340   -8.268844   \n",
       "164  -12.565029 -36.956093  -51.224533 -42.695961  -13.396852  -62.568878   \n",
       "165   -6.807866 -39.883553  -82.117966  -7.156143  -45.256710  -34.827076   \n",
       "166    5.955750  -2.785596   10.657142  -7.704358   -7.466293   -4.669672   \n",
       "167   17.587795  20.504135  -12.424165  -5.677304   13.834739   -8.012489   \n",
       "\n",
       "           997         998        999  \n",
       "0    -5.200476   -6.953192  37.496685  \n",
       "1   -73.859734 -218.618866 -86.510498  \n",
       "2    10.153909   -7.466252 -87.169029  \n",
       "3   -34.405991   -0.702034 -62.810215  \n",
       "4   -53.210026   11.932649 -37.241249  \n",
       "..         ...         ...        ...  \n",
       "163 -69.221214  -25.744562  16.998856  \n",
       "164 -81.089882  -69.127541 -26.845081  \n",
       "165   1.890019  -35.873177 -10.288158  \n",
       "166  -1.998870    7.070156  31.475857  \n",
       "167  -1.515846   21.004345 -11.699253  \n",
       "\n",
       "[168 rows x 1002 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Perform inner join on key and pref_img dataframes, removing non-intersecting images\n",
    "pref_img = key.set_index('Imagenet_id').join(pref_img.set_index('Imagenet_id')).reset_index()\n",
    "\n",
    "#Drop unnecessary columns to prepare the dataframe to be used\n",
    "pref_img = pref_img.drop(['Image_description', 'Alexnet_id'], axis = 1)\n",
    "\n",
    "#pref_img df now is 168 entries (21*8) by 1000 features + 2 metadata columns\n",
    "pref_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38324ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Added fixer to properly construct the indices. I don't know how to fix it properly. \n",
    "#I think having rois and layers with the same number of items per list is the source of the problem. I don't know why.\n",
    "\n",
    "#When the length of ROI and layer lists is the same, the multi-index \n",
    "#is arranged so that each ROI has 8 copies of the same layer instead of 1 copy of each layer. Sorting the index\n",
    "#does not solve this problem.\n",
    "\n",
    "rois = ['VC', 'V1', 'V2', 'V3', 'V4', 'PPA', 'LOC', 'FFA', 'fixer'] \n",
    "\n",
    "sum_index = create_multi_index(x = 1, indices = [rois, layers])\n",
    "sum_col = ['Top 1% Accuracy', 'Top 5% Accuracy', 'Corr-coef'] \n",
    "\n",
    "#df to contain bfd methods' results\n",
    "summary_bfd = pd.DataFrame(index = sum_index, columns = sum_col).sort_index().drop('fixer')\n",
    "\n",
    "sum_col = ['1% SVM', '5% SVM', 'Corr-SVM', '1% logreg', '5% logreg', 'Corr-logreg']\n",
    "sum_index = create_multi_index(x = 1, indices = [rois, layers, subjs])\n",
    "\n",
    "#df to contain other methods' results\n",
    "summary_others = pd.DataFrame(index = sum_index, columns = sum_col).sort_index().drop('fixer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eefd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating top 1% accuracy, top 5% accuracy, and corr coef for the BFD method and others per roi:\n",
    "for roi in rois:\n",
    "    \n",
    "    #Fix for the problem mentioned above.\n",
    "    if roi == 'fixer':\n",
    "        break\n",
    "        \n",
    "    roi_df = data_extractor(roi = roi, p = 'bfd/best_features/1000/', key = key, layers = layers, subjs = subjs)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for layer in layers:\n",
    "            for subj in subjs:\n",
    "                \n",
    "                df = roi_df[(roi_df.Subject == subj) & (roi_df.Layer == layer)]\n",
    "                #Split data into training and test data for the models\n",
    "                #test_size was selected to be 5 repetitions for each image (5 * 21 = 105)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, 3:], df.iloc[:, 0], test_size = 105, random_state = random_seed)\n",
    "    \n",
    "                #SVM\n",
    "                svm = SVC(kernel = 'poly', probability = True)\n",
    "                svm.fit(X_train, y_train)\n",
    "                svm_y_score = svm.predict_proba(X_test)\n",
    "                \n",
    "                #Results\n",
    "                summary_others.loc[roi, layer, subj]['Corr-SVM'] = np.corrcoef(y_test, svm.predict(X_test))[0][1]\n",
    "                summary_others.loc[roi, layer, subj]['1% SVM'] = top_k_accuracy_score(y_test, svm_y_score, k = 1)\n",
    "                summary_others.loc[roi, layer, subj]['5% SVM'] = top_k_accuracy_score(y_test, svm_y_score, k = 5)\n",
    "    \n",
    "                #LogReg\n",
    "                model = LogisticRegression(penalty = 'l1', solver = 'saga', random_state = random_seed, max_iter = n_iter)\n",
    "                model.fit(X_train, y_train)    \n",
    "                model_y_score = model.predict_proba(X_test)\n",
    "                #Results\n",
    "                summary_others.loc[roi, layer, subj]['Corr-logreg'] = np.corrcoef(y_test, model.predict(X_test))[0][1]\n",
    "                summary_others.loc[roi, layer, subj]['1% logreg'] = top_k_accuracy_score(y_test, model_y_score, k = 1)\n",
    "                summary_others.loc[roi, layer, subj]['5% logreg'] = top_k_accuracy_score(y_test, model_y_score, k = 5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #BFD method\n",
    "    corr_roi = np.corrcoef(pref_img.iloc[:, 2:].astype('float'),\n",
    "                           roi_df.iloc[:, 3:].astype('float'))\n",
    "    multi_index = [np.array(roi_df['Imagenet_id']),\n",
    "                   np.array(roi_df['Layer']),\n",
    "                   np.array(roi_df['Subject'])]\n",
    "    multi_column = [np.array(pref_img.Imagenet_id),\n",
    "                    np.array(pref_img.layer)]\n",
    "    \n",
    "    #Running the corrcoef on whole dfs and slicing it this way extracts all the data from the correlation matrix\n",
    "    #Meaning we don't need to use for loops. I confirmed this through experimentation (on VC only).\n",
    "    corr_roi = pd.DataFrame(corr_roi[:168, 168:].T, index = multi_index, columns = multi_column)\n",
    "    \n",
    "    columns = ['is in Top-1?', 'is in Top-5?', 'Corr-coef']\n",
    "    index = create_multi_index (x = roi_df.shape[0]/35, indices = [Imagenet_id, layers, subjs])\n",
    "    results = pd.DataFrame(index = index, columns=columns).sort_index()\n",
    "\n",
    "    #Calculate the results for the roi, and put them in the summary dataframe\n",
    "    for idx in Imagenet_id:\n",
    "        for layer in layers:\n",
    "            for subj in subjs:\n",
    "                results.loc[:,'is in Top-1?'][idx][layer][subj] = idx in (corr_roi.loc[:, idx][layer][sli[:, layer, subj]].sort_values(ascending=False)[:1])\n",
    "        \n",
    "                results.loc[:,'is in Top-5?'][idx][layer][subj] = idx in (corr_roi.loc[:, idx][layer][sli[:, layer, subj]].sort_values(ascending=False)[:5])\n",
    "            \n",
    "                results.loc[:, 'Corr-coef'][idx][layer][subj] = corr_roi.loc[:, idx][layer][idx][layer][subj].mean()\n",
    "    \n",
    "    for layer in layers:\n",
    "        summary_bfd.loc[roi].loc[ layer,'Top 1% Accuracy'] = (results.loc[sli[:, layer, :] , 'is in Top-1?'].sum()/\n",
    "                                                       results.loc[sli[:, layer, :] , 'is in Top-1?'].count())\n",
    "        summary_bfd.loc[roi].loc[ layer,'Top 5% Accuracy'] = (results.loc[sli[:, layer, :] , 'is in Top-5?'].sum()/\n",
    "                                                       results.loc[sli[:, layer, :] , 'is in Top-5?'].count())\n",
    "        summary_bfd.loc[roi].loc[ layer, 'Corr-coef'] = results.loc[sli[:, layer, :], 'Corr-coef'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e36d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results.\n",
    "summary_bfd.reset_index().to_csv('bfd.csv')\n",
    "summary_others.reset_index().to_csv('others.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142718d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
